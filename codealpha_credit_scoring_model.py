# -*- coding: utf-8 -*-
"""CodeAlpha_Credit Scoring Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13WvF9CGehkKpU_2Um6X81WUYTjL-AEn8

**TASK 1 - CREDIT SCORING MODEL (GERMAN CREDIT DATASET)**

**Importation of Necessary Libraries**
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

"""**Loading the German Credit Dataset**"""

url = "https://raw.githubusercontent.com/selva86/datasets/master/GermanCredit.csv"
df = pd.read_csv(url)

"""**Data Preview**"""

df['purpose'].value_counts()

df.head()

df.describe()

df.tail()

df.info()

df.columns

df.describe()

"""**Data Cleaning & Preprocessing**"""

# Check for missing values
print("Check for missing values: ")
print(df.isnull().sum())

# 3.2 Identify categorical columns
cat_cols = df.select_dtypes(include=['object']).columns

# Drop target label ONLY if it is in the list
if "credit_risk" in cat_cols:
    cat_cols = cat_cols.drop("credit_risk")

df_encoded = df.copy()

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df_encoded, columns=cat_cols, drop_first=True)

# Encode target variable (Good = 1, Bad = 0)
if df_encoded['credit_risk'].dtype == 'object':
    label_encoder = LabelEncoder()
    df_encoded['credit_risk'] = label_encoder.fit_transform(df_encoded['credit_risk'])

# Split data
X = df_encoded.drop('credit_risk', axis=1)
y = df_encoded['credit_risk']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)

df_encoded.head()

"""**Training & Evaluation**"""

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)   # create model object; increase max_iter to ensure convergence
log_reg.fit(X_train, y_train)                 # train the model on the training set
y_pred_log = log_reg.predict(X_test)          # predicted labels for test set
y_proba_log = log_reg.predict_proba(X_test)[:, 1]  # predicted probabilities for positive class (used for AUC / ROC)

# Decision Tree Classifier
dtc = DecisionTreeClassifier(random_state=42)  # deterministic results with random_state
dtc.fit(X_train, y_train)
y_pred_dt = dtc.predict(X_test)
y_proba_dt = dtc.predict_proba(X_test)[:, 1]

# Random Forest Classifier
rf = RandomForestClassifier(n_estimators=200, random_state=42)  # 200 trees for stability
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:, 1]

# Evaluation helper
def print_evaluation(name, y_true, y_pred, y_proba):
    print(f"===== {name} =====")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Classification Report:")
    print(classification_report(y_true, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    try:
        print("ROC-AUC:", roc_auc_score(y_true, y_proba))
    except:
        print("ROC-AUC: cannot compute (probabilities missing)")
    print("\n")

# Print evaluations for each model
print_evaluation("Logistic Regression", y_test, y_pred_log, y_proba_log)
print_evaluation("Decision Tree", y_test, y_pred_dt, y_proba_dt)
print_evaluation("Random Forest", y_test, y_pred_rf, y_proba_rf)

# ROC Curve plot
fpr_log, tpr_log, _ = roc_curve(y_test, y_proba_log)
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)

plt.figure(figsize=(8,6))
plt.plot(fpr_log, tpr_log, label=f'Logistic (AUC={roc_auc_score(y_test,y_proba_log):.3f})')
plt.plot(fpr_dt, tpr_dt, label=f'DecisionTree (AUC={roc_auc_score(y_test,y_proba_dt):.3f})')
plt.plot(fpr_rf, tpr_rf, label=f'RandomForest (AUC={roc_auc_score(y_test,y_proba_rf):.3f})')
plt.plot([0,1], [0,1], 'k--', label='Chance')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid(True)
plt.show()

# Save the best model
joblib.dump(rf, "random_forest_german_credit.joblib")
print("Saved Random Forest to random_forest_german_credit.joblib")

# Extract feature importances
importances = rf.feature_importances_
feature_names = X.columns

# Create a Series for easy sorting
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

# Display the values
print("Top Feature Importances:")
print(feat_imp.head(20))

# Plot
plt.figure(figsize=(10, 6))
feat_imp.head(20).plot(kind="bar")
plt.title("Top 20 Feature Importances (Random Forest)")
plt.ylabel("Importance Score")
plt.xlabel("Features")
plt.tight_layout()
plt.show()

